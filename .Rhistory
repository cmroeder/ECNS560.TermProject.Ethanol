add_model(model_net)
#it chooses parameters itself
cv_net =
workflow_net |>
tune_grid(
housing_cv,
grid = grid_regular(mixture(), penalty(), levels=5:5),
metrics = metric_set(rmse)
)
#plotting
autoplot(cv_net, metric = "rmse")
#looking at best 5 models
cv_net |> show_best()
#saving best ridge
final_net =
workflow_net |>
finalize_workflow(select_best(cv_net, metric = "rmse"))
final_fit_net = final_net |> fit(data=housing_train) |> last_fit(split=housing_split)
final_fit_net |> collect_metrics()
housing_recipe =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(log_Sale_Price ~ ., data = housing_train) |>
# Create log terms for numeric predictors
step_log(Lot_Area, First_Flr_SF, Gr_Liv_Area) |>
# Normalize log modified vars
step_normalize(all_numeric_predictors()) |>
#Dummies for nominal vars
step_dummy(all_nominal_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
#getting cleaned data
system.time({
housing_clean <- housing_recipe |> prep() |> juice()
})
#Defining model
model_lm =
linear_reg() |>
set_engine("lm")
#Defining workflow
workflow_lm = workflow() |>
add_model(model_lm) |>
add_recipe(housing_recipe)
#Predicting values
ols= workflow_lm |>
fit(data=housing_train) |>
last_fit(split=housing_split)
#assessing performance
ols |>
collect_metrics(yardstick::rmse, yardstick::mae)
#defining model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -7, to = 0, length = 1e3)
#creating v-fold cross-validation
housing_cv = housing_train |> vfold_cv(v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
library(pacman)
p_load(tidyverse, tidymodels, skimr, glmnet, kknn)
data(ames)
hist(ames$Sale_Price, xlab = "", col = "pink", border = "black")
ames$log_Sale_Price=log(ames$Sale_Price)
hist(ames$log_Sale_Price, xlab = "", col = "grey", border = "black")
#deleting Price as we will use log from now on
ames <- select(ames, -Sale_Price)
# Set a seed (ensures reproducible results)
set.seed(42)
# Create an 80/20 split by random sampling
housing_split = ames |> initial_split(prop = 0.8)
# Grab each subset
housing_train = housing_split |> training()
housing_test  = housing_split |> testing()
housing_split
library(dplyr)
#function to get summary for all variables before recipe
missing_summary = function(data) {
missing_count = sapply(data, function(x) sum(is.na(x)))
variable_types = sapply(data, class)
df <- data.frame(variable = names(data),
missing_count = missing_count,
variable_type = variable_types)
return(df)
}
#applying function
summary_table = missing_summary(housing_train)
print(summary_table)
# Select only numeric and integer variables - we have 34 of them
numeric_vars = sapply(housing_train, is.numeric)
numeric_data = housing_train[, numeric_vars]
numeric_var_names = names(numeric_vars[numeric_vars])
#organizing graphs to 7 rows and 5 columns
par(mfrow = c(7, 5),mar = c(2, 2, 1, 1))
#histograms for each numeric variable
for (var in numeric_var_names) {
hist(housing_train[[var]], main = var, xlab = "", col = "skyblue", border = "black")
}
#Reset the layout to the default
par(mfrow = c(1, 1))
#log transformation of numeric variables
log_transformed_data <- log1p(numeric_data)
#layout to fit all hists
par(mfrow = c(7, 5), mar = c(2, 2, 1, 1))
# Create histograms for each log-transformed numeric variable
for (var in numeric_var_names) {
#handle NAs in log
if (any(!is.na(log_transformed_data[[var]]))) {
hist(log_transformed_data[[var]], main = paste("Log(", var, ")", sep = ""), xlab = "", col = "lightpink", border = "black", breaks = 20)
}
}
# Reset the layout to the default
par(mfrow = c(1, 1))
housing_recipe =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(log_Sale_Price ~ ., data = housing_train) |>
# Create log terms for numeric predictors
step_log(Lot_Area, First_Flr_SF, Gr_Liv_Area) |>
# Normalize log modified vars
step_normalize(all_numeric_predictors()) |>
#Dummies for nominal vars
step_dummy(all_nominal_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
#getting cleaned data
system.time({
housing_clean <- housing_recipe |> prep() |> juice()
})
#Defining model
model_lm =
linear_reg() |>
set_engine("lm")
#Defining workflow
workflow_lm = workflow() |>
add_model(model_lm) |>
add_recipe(housing_recipe)
#Predicting values
ols= workflow_lm |>
fit(data=housing_train) |>
last_fit(split=housing_split)
#assessing performance
ols |>
collect_metrics(yardstick::rmse, yardstick::mae)
# Predicting values on the training set
ols_fit <- workflow_lm %>%
fit(data = housing_train)
predicted_values <- predict(ols_fit, new_data = housing_train)
# Plotting predicted vs actual values
library(ggplot2)
predicted_values$actuals=housing_train$log_Sale_Price
ggplot(data=predicted_values, aes(x = actuals, y = .pred)) +
geom_point() +
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
labs(title = "Predicted vs Actual Values",
x = "Actual log_Sale_Price",
y = "Predicted log_Sale_Price") +
theme_minimal()
#defining model
model_lasso =
linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
#setting lambdas
#Values range from 10^-7 because from iterative process it seems that lower alphas perform better
lambdas = 10 ^ seq(from = -7, to = 0, length = 1e3)
#creating v-fold cross-validation
housing_cv = housing_train |> vfold_cv(v = 5)
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
library(ggplot2)
all_lasso=lasso_cv |> show_best(n=1000) #get all models
#plotting
library(ggplot2)
#plotting standard error which is variance
ggplot(all_lasso, aes(x = penalty, y=std_err)) +
geom_point(color = "blue", size = 1)
#plotting mean rmse which is bias
ggplot(all_lasso, aes(x = penalty)) +
geom_point(aes(y = mean), color = "pink", size = 1)
autoplot(lasso_cv, metric="rmse")
#seeing how the best parameter performed in test data
final_fit_lasso = final_lasso |> fit(data=housing_train) |> last_fit(split=housing_split)
#getting the coefficients from best models
coefs = final_fit_lasso |>
extract_fit_parsnip() |>
tidy()
#getting non-zero coefs
coefs_nonzero = coefs |>
filter(estimate > 0) |>
arrange(desc(estimate))
#creating lists of good and not good predictors
coef_lasso = coefs_nonzero$term
all_coefs=coefs$term
coef_lasso <- coef_lasso[coef_lasso != "(Intercept)"]
coefs_nonzero
print(setdiff(all_coefs, coef_lasso))
final_fit_lasso |> collect_metrics()
library(recipes)
library(parsnip)
housing_recipe1 =
# Define the recipe: Rating predicted by all other vars in housing_train
recipe(log_Sale_Price ~ ., data = housing_train) |>
# Create log terms for numeric predictors
#step_log(Lot_Area, First_Flr_SF, Gr_Liv_Area) |>
# Normalize log modified vars
step_normalize(all_numeric_predictors()) |>
step_poly(all_numeric_predictors(), degree=2) |>
#Dummies for nominal vars
step_dummy(all_nominal_predictors()) |>
#update_role(all_predictors(), roles = (predictor = coef_lasso)) |>
#step_select(all_numeric_predictors()) |>
# Remove predictors with near-zero variance (improves stability)
step_nzv(all_numeric_predictors()) |>
# Remove very collinear vars
step_corr(all_numeric_predictors(), method = "spearman",threshold = 0.9)
#getting cleaned data
system.time({
housing_clean <- housing_recipe1 |> prep() |> juice()
})
model_ridge =
linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
#setting lambdas
lambdas = 10 ^ seq(from = -5, to = 2, length = 1e3)
#creating v-fold cross-validation
housing_cv = housing_train |> vfold_cv(v = 5)
#defining workflow
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(housing_recipe1)
#tuning for a Ridge regression model
ridge_cv = workflow_ridge |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#plotting rmse against alpha
autoplot(ridge_cv, metric = "rmse")
#looking at best 5 models
ridge_cv |> show_best()
#saving best ridge
final_ridge =
workflow_ridge |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
#define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) |>
set_engine("glmnet")
#define the workflow
workflow_net = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_net)
#it chooses parameters itself
cv_net =
workflow_net |>
tune_grid(
housing_cv,
grid = grid_regular(mixture(), penalty(), levels=5:5),
metrics = metric_set(rmse)
)
#plotting
autoplot(cv_net, metric = "rmse")
#define the elasticnet model
model_net = linear_reg(penalty = tune(), mixture = tune()) |>
set_engine("glmnet")
#define the workflow
workflow_net = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_net)
#it chooses parameters itself
cv_net =
workflow_net |>
tune_grid(
housing_cv,
grid = grid_regular(mixture(), penalty(), levels=5:5),
metrics = metric_set(rmse)
)
#plotting
autoplot(cv_net, metric = "rmse")
#looking at best 5 models
cv_net |> show_best()
#saving best ridge
final_net =
workflow_net |>
finalize_workflow(select_best(cv_net, metric = "rmse"))
# Define a simple KNN model
model_knn = nearest_neighbor(neighbors = tune(), mode = "regression") %>%
set_engine("kknn", scale = TRUE)
#define the workflow
workflow_knn = workflow() |>
add_recipe(housing_recipe1) |>
add_model(model_knn)
#tuning
knn_cv =
workflow_knn |>
tune_grid(
resamples = housing_cv,
grid = data.frame(neighbors = seq(5, 20)),# Example grid for tuning k
metrics = metric_set(rmse)
)
#looking at best 5 models
knn_cv |> show_best()
#plotting
autoplot(knn_cv, metrics="rmse")
#saving best KNN
final_knn =
workflow_knn |>
finalize_workflow(select_best(knn_cv, metric = "rmse"))
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe1)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
#defining workflow
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(housing_recipe1)
#tuning for a Lasso regression model
lasso_cv = workflow_lasso |>
tune_grid(
housing_cv,
grid = data.frame(penalty = lambdas),
metrics = metric_set(yardstick::rmse)
)
#get the best lasso
lasso_cv |> show_best()
#the best parameter
final_lasso =
workflow_lasso |>
finalize_workflow(select_best(lasso_cv, metric = "rmse"))
final_lasso
final_fit_lasso |> collect_metrics()
library(ggplot2)
library(maps)
library(stringr)
library(tidyr)
library(dplyr)
library(readxl)
library(outliers)
#reading final
merge_final=read.csv("Data/Merging/merge_final_final1.csv")
#libraries
library(readxl)
library(dplyr)
library(dplyr)
#path to file
excel_file <- "Data/Raw/historical-station-counts1.xlsx"
#list of all years because sheets are named after years
sheet_names <- as.character(seq(2007, 2019, by = 1))
#making a function
all_data <- lapply(sheet_names, function(sheet) {
data <- read_excel(excel_file, sheet = sheet, skip = 1)  # Skip first row, read header row as column names
data <- data[!is.na(data[[1]]), ]  # Remove rows where the first column is NA
data <- data[!is.na(data$E85), ]   # Remove rows where 'E85' column is NA
data$year <- sheet  # Add 'year' column with sheet name
return(data)
})
# Vertically concatenate all data frames into one
combined_data <- bind_rows(all_data)
#libraries
library(readxl)
library(dplyr)
library(dplyr)
#path to file
excel_file <- "Data/Raw/historical-station-counts1.xlsx"
#list of all years because sheets are named after years
sheet_names <- as.character(seq(2007, 2019, by = 1))
#making a function
all_data <- lapply(sheet_names, function(sheet) {
data <- read_excel(excel_file, sheet = sheet, skip = 1)  # Skip first row, read header row as column names
data <- data[!is.na(data[[1]]), ]  # Remove rows where the first column is NA
data <- data[!is.na(data$E85), ]   # Remove rows where 'E85' column is NA
data$year <- sheet  # Add 'year' column with sheet name
return(data)
})
# Vertically concatenate all data frames into one
combined_data <- bind_rows(all_data)
setwd("C:/Users/aitku/OneDrive/Рабочий стол/Fall 2023/Advanced_Data_Analysis/GitHub/ECNS560.TermProject.Ethanol/")
#libraries
library(readxl)
library(dplyr)
library(dplyr)
#path to file
excel_file <- "Data/Raw/historical-station-counts1.xlsx"
#list of all years because sheets are named after years
sheet_names <- as.character(seq(2007, 2019, by = 1))
#making a function
all_data <- lapply(sheet_names, function(sheet) {
data <- read_excel(excel_file, sheet = sheet, skip = 1)  # Skip first row, read header row as column names
data <- data[!is.na(data[[1]]), ]  # Remove rows where the first column is NA
data <- data[!is.na(data$E85), ]   # Remove rows where 'E85' column is NA
data$year <- sheet  # Add 'year' column with sheet name
return(data)
})
# Vertically concatenate all data frames into one
combined_data <- bind_rows(all_data)
#making function between 2020 and 2022 because data structure differs a little bit
sheet_names1 <- as.character(seq(2020, 2022, by = 1))
all_data1 <- lapply(sheet_names1, function(sheet) {
data1 <- read_excel(excel_file, sheet = sheet, skip = 1)  # Skip first row, read header row as column names
data1 <- data1[!is.na(data1[[1]]), ]  # Remove rows where the first column is NA
data1 <- data1[!is.na(data1$E85), ]   # Remove rows where 'E85' column is NA
data1$year <- sheet  # Add 'year' column with sheet name
return(data1)
})
# Vertically concatenate all data frames into one
combined_data1 <- bind_rows(all_data1)
#selecting only necessary columns
# View the combined data
years_2009_2019=combined_data |>
select(State,year,E85,Total)
years_2020_2022=combined_data1 |>
select(State, year, E85, Totald)
#renaming columns
colnames(years_2009_2019) <- c("state", "year", "e85", "total")
colnames(years_2020_2022) <- c("state", "year", "e85", "total")
#concating vertically
e85_df <- rbind(years_2009_2019, years_2020_2022)
#saving the df
e85_df$ratio_e85=e85_df$e85/e85_df$total*100
write.csv(e85_df, "Data/Cleaning/e_85.csv")
#merging everything
file_path <- "Data/Cleaning/state_names.csv"
state_names <- read_csv(file_path)
#merging everything
file_path <- "Data/Cleaning/state_names.csv"
state_names <- read_csv(file_path)
#merging everything
file_path <- "Data/Cleaning/state_names.csv"
state_names <- read.csv(file_path)
regulations_laws_final=read.csv("Data/Cleaning/regulations_laws_final.csv")
#merging abbr and laws
names(regulations_laws_final)[names(regulations_laws_final) == 'State'] <- 'Alpha code'
regulations_laws_merged=merge(x = regulations_laws_final, y = state_names, by = "Alpha code", all.x = TRUE)
file_path <- "Data/Cleaning/state_names.csv"
state_names <- read_csv(file_path)
file_path <- "Data/Cleaning/state_names.csv"
state_names <- read.csv(file_path)
regulations_laws_final=read.csv("Data/Cleaning/regulations_laws_final.csv")
e85_df=read.csv("Data/Cleaning/e_85.csv")
names(regulations_laws_final)[names(regulations_laws_final) == 'State'] <- 'Alpha code'
regulations_laws_merged=merge(x = regulations_laws_final, y = state_names, by = "Alpha code", all.x = TRUE)
View(regulations_laws_final)
names(regulations_laws_final)[names(regulations_laws_final) == 'State'] <- 'Alpha code'
regulations_laws_merged=merge(x = regulations_laws_final, y = state_names, by = "Alpha code", all.x = TRUE)
